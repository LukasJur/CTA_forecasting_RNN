{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing general parameters\n",
    "import os\n",
    "import re\n",
    "path_to_CTA = '/home/jupyter/CTA data/'\n",
    "path_to_params = '/home/jupyter/CTA params/'\n",
    "cutoff_date = '2006-12-01'\n",
    "fileNames = os.listdir(path_to_CTA)\n",
    "fileNames = list(filter(lambda x: x.endswith('.csv'), fileNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take crude oil data\n",
    "oil = pd.read_csv(path_to_params + 'MCOILWTICO.csv')\n",
    "oil.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "oil['DATE'] = pd.to_datetime(oil['DATE'], format='%Y-%m')\n",
    "oil = oil.set_index('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take gold data\n",
    "aux = pd.read_csv(path_to_params + 'AUX-USD-2592000-20200523172420.csv', parse_dates=['Date'])\n",
    "aux.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "aux['Date'] = aux['Date'].apply(lambda dt: dt.replace(day=1))\n",
    "aux.drop_duplicates(subset =\"Date\", \n",
    "                     keep = 'first', inplace = True)\n",
    "aux = aux.set_index('Date')\n",
    "aux = aux.reindex(index=aux.index[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take silver data\n",
    "agx = pd.read_csv(path_to_params + 'AGX-USD-2592000-20200523172540.csv', parse_dates=['Date'])\n",
    "agx.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "agx['Date'] = agx['Date'].apply(lambda dt: dt.replace(day=1))\n",
    "agx.drop_duplicates(subset =\"Date\", \n",
    "                     keep = 'first', inplace = True)\n",
    "agx = agx.set_index('Date')\n",
    "agx = agx.reindex(index=agx.index[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Producer Price index data\n",
    "ppi = pd.read_csv(path_to_params + 'PPIACO.csv', parse_dates=['DATE'])\n",
    "ppi.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "ppi.drop_duplicates(subset =\"DATE\", \n",
    "                     keep = 'first', inplace = True)\n",
    "ppi = ppi.set_index('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "# Take BTOP50 index data\n",
    "btop = pd.read_csv(path_to_params + 'BTOP50_Index_historical_data.csv')\n",
    "btop = btop.iloc[:34]\n",
    "btop_df = pd.DataFrame(columns=['Date', 'BTOP50'])\n",
    "# print(btop_df)\n",
    "for index, row in btop.iterrows():\n",
    "    for idx, value in row[1:].items():\n",
    "        year = int(row[0])\n",
    "        month = list(calendar.month_abbr).index(idx)\n",
    "        actual_date = datetime(year=year, month=month, day=1)\n",
    "        btop_df = btop_df.append({'Date':actual_date,'BTOP50':value},ignore_index=True)\n",
    "        btop_df = btop_df[btop_df['Date'] < '2020-05-01']\n",
    "        btop_df = btop_df[btop_df['Date'] >= cutoff_date]\n",
    "\n",
    "btop_df['BTOP50'] = btop_df['BTOP50'].apply(lambda x: x.strip('%')).astype(float)\n",
    "btop_df = btop_df.set_index('Date') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Take CTA index data\n",
    "ctai_list = (open(path_to_params + \"cta_index_formated.txt\", \"r\").read())\n",
    "ctai_list = ast.literal_eval(ctai_list)\n",
    "ctai_df = pd.DataFrame.from_records(ctai_list)\n",
    "ctai_df['Date'] = pd.to_datetime(ctai_df['year'].astype(str) + '-' + ctai_df['month'].astype(str), format='%Y-%m')\n",
    "ctai_df = ctai_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTAs = {}\n",
    "# Take CTA data\n",
    "for fName in fileNames:\n",
    "    df = pd.read_csv(path_to_CTA + fName)\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    df[\"Month\"] = df.Month.map(\"{:02}\".format)\n",
    "    df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "    df = df.set_index('Date')\n",
    "    df = df.loc[cutoff_date:]\n",
    "    df.drop(columns=['Year','Month'], inplace=True)\n",
    "    # Insert params to df\n",
    "    df['Oil'] = oil.loc[cutoff_date:]['MCOILWTICO']\n",
    "    df['AUX'] = aux.loc[cutoff_date:]['Close (kg)']\n",
    "    df['AGX'] = agx.loc[cutoff_date:]['Close (kg)']\n",
    "    df['PPI'] = ppi.loc[cutoff_date:]['PPIACO']\n",
    "    df['BTOP50'] = btop_df['BTOP50']\n",
    "    df['CTA_IDX'] = ctai_df.loc[cutoff_date:]['value']\n",
    "    CTAs[fName] = df.loc[:'2020-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Oil</th>\n",
       "      <th>AUX</th>\n",
       "      <th>AGX</th>\n",
       "      <th>PPI</th>\n",
       "      <th>BTOP50</th>\n",
       "      <th>CTA_IDX</th>\n",
       "      <th>DataSet_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-01</th>\n",
       "      <td>1.47</td>\n",
       "      <td>120500000</td>\n",
       "      <td>61.96</td>\n",
       "      <td>20077.35</td>\n",
       "      <td>412.82</td>\n",
       "      <td>165.6</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.94</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>0.47</td>\n",
       "      <td>119800000</td>\n",
       "      <td>54.51</td>\n",
       "      <td>21498.71</td>\n",
       "      <td>449.47</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.02</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-01</th>\n",
       "      <td>1.55</td>\n",
       "      <td>113200000</td>\n",
       "      <td>59.28</td>\n",
       "      <td>21028.07</td>\n",
       "      <td>420.21</td>\n",
       "      <td>166.8</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-01</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>127600000</td>\n",
       "      <td>60.44</td>\n",
       "      <td>22205.52</td>\n",
       "      <td>453.00</td>\n",
       "      <td>169.3</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-01</th>\n",
       "      <td>1.70</td>\n",
       "      <td>223700000</td>\n",
       "      <td>63.98</td>\n",
       "      <td>21298.72</td>\n",
       "      <td>423.10</td>\n",
       "      <td>171.4</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5.81</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-01</th>\n",
       "      <td>-3.51</td>\n",
       "      <td>222900000</td>\n",
       "      <td>45.48</td>\n",
       "      <td>35810.90</td>\n",
       "      <td>467.80</td>\n",
       "      <td>189.1</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-01</th>\n",
       "      <td>1.38</td>\n",
       "      <td>232600000</td>\n",
       "      <td>46.22</td>\n",
       "      <td>36717.92</td>\n",
       "      <td>500.19</td>\n",
       "      <td>187.5</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-01</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>233300000</td>\n",
       "      <td>42.44</td>\n",
       "      <td>34219.00</td>\n",
       "      <td>452.88</td>\n",
       "      <td>185.7</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.23</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-01</th>\n",
       "      <td>0.18</td>\n",
       "      <td>240000000</td>\n",
       "      <td>37.19</td>\n",
       "      <td>35951.61</td>\n",
       "      <td>458.67</td>\n",
       "      <td>183.5</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>-1.49</td>\n",
       "      <td>241600000</td>\n",
       "      <td>31.68</td>\n",
       "      <td>39270.13</td>\n",
       "      <td>473.84</td>\n",
       "      <td>182.6</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.45</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Return     Assets    Oil       AUX     AGX    PPI  BTOP50 CTA_IDX  \\\n",
       "Date                                                                            \n",
       "2006-12-01    1.47  120500000  61.96  20077.35  412.82  165.6    2.08    1.94   \n",
       "2007-01-01    0.47  119800000  54.51  21498.71  449.47  164.0    1.12    2.02   \n",
       "2007-02-01    1.55  113200000  59.28  21028.07  420.21  166.8   -1.63   -1.90   \n",
       "2007-03-01   -0.04  127600000  60.44  22205.52  453.00  169.3   -1.10   -1.49   \n",
       "2007-04-01    1.70  223700000  63.98  21298.72  423.10  171.4    2.35    5.81   \n",
       "...            ...        ...    ...       ...     ...    ...     ...     ...   \n",
       "2015-09-01   -3.51  222900000  45.48  35810.90  467.80  189.1    1.73    1.20   \n",
       "2015-10-01    1.38  232600000  46.22  36717.92  500.19  187.5   -1.16   -1.36   \n",
       "2015-11-01   -0.07  233300000  42.44  34219.00  452.88  185.7    2.88    2.23   \n",
       "2015-12-01    0.18  240000000  37.19  35951.61  458.67  183.5   -1.61   -0.92   \n",
       "2016-01-01   -1.49  241600000  31.68  39270.13  473.84  182.6    2.87    1.45   \n",
       "\n",
       "           DataSet_1  \n",
       "Date                  \n",
       "2006-12-01     Train  \n",
       "2007-01-01     Train  \n",
       "2007-02-01     Train  \n",
       "2007-03-01     Train  \n",
       "2007-04-01     Train  \n",
       "...              ...  \n",
       "2015-09-01     Train  \n",
       "2015-10-01     Train  \n",
       "2015-11-01     Train  \n",
       "2015-12-01     Train  \n",
       "2016-01-01     Train  \n",
       "\n",
       "[110 rows x 9 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_start = '2006-12-01'\n",
    "train_end = '2016-01-01' # inclusive\n",
    "test_end = '2020-04-01' # inclusive\n",
    "\n",
    "for fName in fileNames:\n",
    "    # Mark as train and test sets\n",
    "    df = CTAs[fName]\n",
    "    df['DataSet_1'] = 'Test'\n",
    "    df.loc[train_start:train_end,'DataSet_1'] = 'Train'\n",
    "    \n",
    "CTAs[fileNames[0]].loc[train_start:train_end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training and testing set\n",
    "transform_train = {}\n",
    "transform_test = {}\n",
    "scaler = {}\n",
    "\n",
    "for num, fName in enumerate(fileNames):\n",
    "    sc = MinMaxScaler(feature_range=(0,1))\n",
    "    df = CTAs[fName]\n",
    "    a0 = np.array(df[df[\"DataSet_1\"] == 'Train'].iloc[:,:-1])\n",
    "    a1 = np.array(df[df[\"DataSet_1\"] == 'Test'].iloc[:,:-1])\n",
    "    transform_train[fName] = sc.fit_transform(a0)\n",
    "    transform_test[fName] = sc.fit_transform(a1)\n",
    "    scaler[fName] = sc\n",
    "\n",
    "# print(transform_train, transform_test)\n",
    "del a0\n",
    "del a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mondiale-Asset-Management-Mondiale-Trading-Program-2X-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'FTC-Capital-GmbH-FTC-Futures-Fund-Classic-EUR-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Global-Bayesian-Dynamics-LLC-SBF-Proprietary-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Molinero-Capital-Management-LLP-Global-Markets-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'FORT-LP-Fort-Global-Contrarian_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Warrington-Asset-Management-Warrington-Strategic-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Claughton-Capital-Institutional-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Kaiser-Trading-Group-Kaiser-Global-Diversified-Program-Class-A_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Hamer-Trading-Diversified-Systematic-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'FORT-LP-Fort-Global-Diversified_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'SMN-Investment-Services-GmbH-smn-Diversified-Futures-Fund-i14-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'John-Locke-Investments-Cyril-Systematic-UCITS-Fund_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'W-G-Wealth-Guardian-Ltd-SAFI2-Program-SAFI2-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Kaiser-Trading-Group-Kaiser-Trading-Fund-2X-SPC-Share-Class-B_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Transtrend-B-V-DTP-Enhanced-Risk-EUR-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Crabel-Capital-Management-Crabel-Multi-Product_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Superfund-Group-Superfund-Green-Master_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'M-S-Capital-Management-Global-Diversified-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Alder-Capital-DAC-Alder-Global-20_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Robust-Methods-LLC-Macro-Managed-Account_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Chesapeake-Capital-Diversified-LV-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Silicon-Valley-Quantitative-Advisors-US-Quantitative-Portfolio-UQP-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'White-Indian-Trading-Co-STAIRS-Proprietary-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Goldman-Management-Inc-Stock-Index-Futures-Prop-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'GammaQ-Breckhurst-Commodity-Fund_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Superfund-Group-Superfund-Green-Gold-B-SPC-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Schindler-Capital-Management-LLC-Dairy-Advantage-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Salus-Alpha-Capital-Ltd-Salus-Alpha-Directional-Markets-DMX-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Rosetta-Capital-Management-Rosetta-Trading-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'EMC-Capital-Advisors-LLC-Classic_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Quantica-Capital-AG-Quantica-Managed-Futures_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Quantitative-Investment-Management-Quantitative-Global-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'SMN-Investment-Services-GmbH-smn-Diversified-Futures-Fund-1996-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Eclipse-Capital-Management-Global-Monetary_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Wimmer-Horizon-LLP-Managed-Account_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Auspice-Capital-Advisors-Ltd-Auspice-Diversified-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'NuWave-Investment-Management-LLC-Combined-Futures-Portfolio-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'RCMA-Capital-LLP-Merchant-Commodity-Fund_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Quest-Partners-LLC-AlphaQuest-Original-program-AQO-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Welton-Investment-Partners-Welton-Global_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Mesirow-Financial-Investment-Management-Inc-Extended-Markets-Alpha-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Crescent-Bay-Capital-Management-Premium-Stock-Index-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Four-Seasons-Commodities-Corp-Hawkeye-Spread-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Eclipse-Capital-Management-Global-Allocation-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Rhicon-Currency-Management-Rhicon-Strategic-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Salus-Alpha-Capital-Ltd-Salus-Alpha-Global-Alpha-GAX-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Quality-Capital-Management-Ltd-Global-Diversified-Programme_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Red-Rock-Capital-LLC-Systematic-Global-Macro_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Transtrend-B-V-DTP-Standard-Risk-EUR-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Kaiser-Trading-Group-Kaiser-Global-Diversified-Program-Class-B_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Estlander-Partners-Alpha-Trend_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Rhicon-Currency-Management-Rhicon-Systematic-Currency-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Parizek-Capital-Futures-Trading-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Tactical-Investment-Management-Tactical-Institutional-Commodity-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Eckhardt-Trading-Company-Evolution-Strategies-1-2X_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Eckhardt-Trading-Company-Evolution-Strategies_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'AIS-Capital-Management-L-P-MAAP-2x-4x-Composite_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'HPX-Financial-LLC-HPX-Old-School-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Emil-van-Essen-Spread-Trading-Program-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'ReSolve-Asset-Management-Inc-Resolve-Acorn-Diversified-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Transtrend-B-V-DTP-Enhanced-Risk-USD-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'DUNN-Capital-Management-D-Best-Futures-Fund-L-P-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'SEB-Asset-Management-SEB-Asset-Selection-Fund-C-EUR-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Silicon-Valley-Quantitative-Advisors-UQP-Small_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Paskewitz-Asset-Management-LLC-Contrarian-S-P-Stock-Index_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Aspect-Capital-Aspect-Diversified-Fund-Class-A-USD-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Southwest-Managed-Investments-LLC-Global-Diversified_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Superfund-Group-Superfund-Green-Gold-A-SPC-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Ansbacher-Investment-Management-Market-Neutral-Put-Skew-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Dreiss-Research-Corporation-Diversified-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Mesirow-Financial-Investment-Management-Inc-Asian-Markets-Alpha-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Chesapeake-Capital-Diversified-Plus-HV-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'NuWave-Investment-Management-LLC-NuWave-Investment-Partners-LP_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Drury-Capital-Diversified-Trend-Following-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'DUNN-Capital-Management-World-Monetary-and-Agriculture-Program-WMA-_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Adalpha-Asset-Management-LLC-Adalpha-Diversified-Short-Term-Program_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Mulvaney-Capital-Management-The-Mulvaney-Global-Markets-Fund_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'AIS-Capital-Management-L-P-MAAP-3x-6x-Composite_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'Superfund-Group-Superfund-Green-Q-AG_data.csv': MinMaxScaler(copy=True, feature_range=(0, 1))}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = {}\n",
    "testset = {}\n",
    "\n",
    "feature_count = 10,\n",
    "train_set_size = 110\n",
    "\n",
    "for fName in fileNames:\n",
    "    trainset[j] = {}\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(feature_count,train_set_size):\n",
    "        X_train.append(transform_train[j][i-feature_count:i,0])\n",
    "        y_train.append(transform_train[j][i,0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    trainset[j][\"X\"] = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "    trainset[j][\"y\"] = y_train\n",
    "    \n",
    "    testset[j] = {}\n",
    "    X_test = []\n",
    "    y_test = []    \n",
    "    for i in range(60, 755):\n",
    "        X_test.append(transform_test[j][i-60:i,0])\n",
    "        y_test.append(transform_test[j][i,0])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "    testset[j][\"X\"] = np.reshape(X_test, (X_test.shape[0], X_train.shape[1], 1))\n",
    "    testset[j][\"y\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fitting to the training set\n",
    "for fName in fileNames:\n",
    "    print(\"Fitting to\", fName)\n",
    "    regressor.fit(trainset[fName][\"X\"], trainset[fName][\"y\"], epochs=50, batch_size=200)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
